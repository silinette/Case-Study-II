#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
# Read the dataset directly from the provided URL
url <- "https://raw.githubusercontent.com/silinette/Case-Study-II/refs/heads/main/Data%20File_Case_Study_Factor%20Analysis_MD.csv"
Data <- read.csv(url, header = TRUE)
Data
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
# Select only variables needed for the analysis
Data <- Data %>%
select(
qd1, qd2, qd3, qd4, qd5, qd6, qd7, qd8, qd9,
qd10, qd11, qd12, qd13, qd14, qd15, qd16, qd17, qd18, qd19,
qd20, qd21, qd22, qd23, qd24, qd25, qd26, qd27, qd28, qd29,
qd30, qd31, qd32, qd33,
brandrec, wtp1, wtp2, wtp3, ri1, ri2
)
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values
# Remove all rows with missing values (listwise deletion)
Data_clean <- na.omit(Data)
# Сheck structure and dimensions of the new dataset
str(Data_clean)
dim(Data_clean)
sum(is.na(Data_clean))
#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)
# Descriptive statistics for 33 quality-related variables
# Descriptive statistics (mean, standard deviation, min, max, etc.) for the 33 quality-related survey items (qd1 to qd33) from the cleaned dataset. The results are rounded to 2 decimal places for readability.
# Normality test not included (norm = TRUE) because Likert-scale items are not truly continuous.
round(stat.desc(Data_clean[, paste0("qd", 1:33)]), digits = 3)
# Reshape only qd1 to qd33 into long format for visualization
data_long <- Data_clean %>%
select(paste0("qd", 1:33)) %>%  # Use only quality-related survey items
pivot_longer(everything(), names_to = "Item", values_to = "Response")  # Convert from wide to long format
# Order items numerically (qd1 to qd33) instead of alphabetically
data_long$Item <- factor(data_long$Item, levels = paste0("qd", 1:33))
# Preview reshaped data
head(data_long) # Show the first few rows of the long-format data to check the transformation
# Convert responses to ordered factor
data_long$Response <- factor(data_long$Response, levels = sort(unique(unlist(Data_clean)))) # Convert all numeric responses into an ordered factor to preserve Likert scale order in plots
# Plot response distributions for all items (directly in R Markdown)
ggplot(data_long, aes(x = Response)) + # Set x-axis to response values
geom_bar(fill = "violetred2") + # Create bar charts
facet_wrap(~ Item, scales = "free_y") + # One plot per item
labs(title = "Distributions", x = "Response", y = "Count") + # Title and axis labels
theme_minimal(base_size = 6)  # Clean theme
# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(as.matrix(raqMatrix))
# Run Bartlett's test of sphericity
bart_spher(Data_clean[, paste0("qd", 1:33)])
#KMO Criterion
kmo_test <- KMOS(Data_clean[, paste0("qd", 1:33)])
kmo_test$KMO           # Overall KMO measure
#Anti-Image Correlations
sort(kmo_test$MSA)     # Measure of sampling adequacy for each item
# Remove qd4 and qd23 due to low multicollinearity
# These two items are weekly correlated and may distort the factor structure, so they are excluded from the analysis.
Data_clean <- Data_clean[, !colnames(Data_clean) %in% c("qd4", "qd23")]
head(Data_clean)
# Running PCA without rotation for scree plot analysis
PC0 <- principal(Data_clean[, paste0("qd", 1:33)],
rotate = "none")
# Remove qd4 and qd23 due to low multicollinearity
# These two items are weekly correlated and may distort the factor structure, so they are excluded from the analysis.
Data_clean <- Data_clean[, !colnames(Data_clean) %in% c("qd4", "qd23")]
head(Data_clean)
# Running PCA without rotation for scree plot analysis
PC0 <- principal(Data_clean[, paste0("qd", 1:33)],
rotate = "none")
# Сlear the database
rm(list=ls())
graphics.off()
#Readability in HTML report
#install.packages("DT")
library(DT)
# Clear the database
rm(list=ls())
#Install necessary packages. Delete # sign in order to download all of them.
#install.packages('rcompanion', repos = "https://cloud.r-project.org")
#install.packages('nortest', repos = "https://cloud.r-project.org")
#install.packages('corrplot', repos = "https://cloud.r-project.org")
#install.packages('olsrr', repos = "https://cloud.r-project.org")
#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
# Read the dataset directly from the provided URL
url <- "https://raw.githubusercontent.com/silinette/Case-Study-II/refs/heads/main/Data%20File_Case_Study_Factor%20Analysis_MD.csv"
Data <- read.csv(url, header = TRUE)
Data
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
# Select only variables needed for the analysis
Data <- Data %>%
select(
qd1, qd2, qd3, qd4, qd5, qd6, qd7, qd8, qd9,
qd10, qd11, qd12, qd13, qd14, qd15, qd16, qd17, qd18, qd19,
qd20, qd21, qd22, qd23, qd24, qd25, qd26, qd27, qd28, qd29,
qd30, qd31, qd32, qd33,
brandrec, wtp1, wtp2, wtp3, ri1, ri2
)
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values
# Remove all rows with missing values (listwise deletion)
Data_clean <- na.omit(Data)
# Сheck structure and dimensions of the new dataset
str(Data_clean)
dim(Data_clean)
sum(is.na(Data_clean))
#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)
# Descriptive statistics for 33 quality-related variables
# Descriptive statistics (mean, standard deviation, min, max, etc.) for the 33 quality-related survey items (qd1 to qd33) from the cleaned dataset. The results are rounded to 2 decimal places for readability.
# Normality test not included (norm = TRUE) because Likert-scale items are not truly continuous.
round(stat.desc(Data_clean[, paste0("qd", 1:33)]), digits = 3)
# Reshape only qd1 to qd33 into long format for visualization
data_long <- Data_clean %>%
select(paste0("qd", 1:33)) %>%  # Use only quality-related survey items
pivot_longer(everything(), names_to = "Item", values_to = "Response")  # Convert from wide to long format
# Order items numerically (qd1 to qd33) instead of alphabetically
data_long$Item <- factor(data_long$Item, levels = paste0("qd", 1:33))
# Preview reshaped data
head(data_long) # Show the first few rows of the long-format data to check the transformation
# Convert responses to ordered factor
data_long$Response <- factor(data_long$Response, levels = sort(unique(unlist(Data_clean)))) # Convert all numeric responses into an ordered factor to preserve Likert scale order in plots
# Plot response distributions for all items (directly in R Markdown)
ggplot(data_long, aes(x = Response)) + # Set x-axis to response values
geom_bar(fill = "violetred2") + # Create bar charts
facet_wrap(~ Item, scales = "free_y") + # One plot per item
labs(title = "Distributions", x = "Response", y = "Count") + # Title and axis labels
theme_minimal(base_size = 6)  # Clean theme
# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(as.matrix(raqMatrix))
# Run Bartlett's test of sphericity
bart_spher(Data_clean[, paste0("qd", 1:33)])
#KMO Criterion
kmo_test <- KMOS(Data_clean[, paste0("qd", 1:33)])
kmo_test$KMO           # Overall KMO measure
#Anti-Image Correlations
sort(kmo_test$MSA)     # Measure of sampling adequacy for each item
# Running PCA without rotation for scree plot analysis
PC0 <- principal(Data_clean[, paste0("qd", 1:33)],
rotate = "none")
PC0$values #Eigenvalues
# Scree plot of eigenvalues
plot(PC0$values, type = "b", main = "Scree Plot", xlab = "Factor Number", ylab = "Eigenvalue")
abline(h = 1) # Kaiser criterion line
# Extraction and displaying communalities
PC0_communalities <- data.frame(sort(PC0$communality))
PC0_communalities
# Principal Components Analysis (PCA) with Varimax Rotation
PC1 <- principal(
Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
rotate = "varimax",
nfactors = 8,
scores = TRUE
)
# Extract eigenvalues
EigenValue_pc1 <- PC1$values
EigenValue_pc1
# Sort and save communalities
PC1_communalities <- data.frame(sort(PC1$communality))
PC1_communalities
# Extraction eigenvalues from the PCA object
EigenValue <- PC1$values
# Number of items (qd1..qd33) used in factor analysis
n_items <- 31
# Calculating percentage of variance explained by each factor
Variance <- EigenValue / n_items * 100
# Cumulative variance explained
SumVariance <- cumsum(Variance)
# Building a summary table of eigenvalues and variance explained
Total_Variance_Explained <- cbind(
EigenValue = EigenValue[EigenValue > 0],
Variance = Variance[Variance > 0],
Total_Variance = SumVariance[Variance > 0]
)
# Viewing the table
Total_Variance_Explained
# Print factor loadings above 0.3, sorted by factor
# A threshold of 0.3 is commonly used to highlight meaningful factor-variable relationships
print(PC1$loadings, cutoff = 0.3, sort = TRUE)
library(knitr)
factor_table <- data.frame(
Factor = c("RC1", "RC2", "RC3", "RC4", "RC5", "RC6", "RC7", "RC8"),
Quality_Dimension = c(
"Performance",
"Serviceability",
"Aesthetics/Appearance",
"Durability",
"Ease of Use",
"Features/Versatility",
"Reliability/Flawlessness",
"Distinctiveness/Prestige"
),
Survey_Items = c(
"qd2, qd5, qd7, qd12, qd16",              # RC1
"qd9, qd14, qd19, qd21, qd24",             # RC2
"qd1, qd10, qd20, qd27",                   # RC3
"qd15, qd17, qd32",                        # RC4
"qd3, qd11, qd13, qd30",                   # RC5
"qd6, qd8, qd18, qd25",                    # RC6
"qd22, qd29, qd33",                        # RC7
"qd26, qd28, qd31"                         # RC8
),
Interpretation = c(
"Core performance and consistency in main functions.",
"Responsiveness and helpfulness of customer service.",
"Design, visual appeal, and overall look of the product.",
"Sturdiness and resistance to wear over time.",
"Easy to understand, navigate, and operate.",
"Advanced features and technical flexibility.",
"Lack of defects and high technical accuracy.",
"Prestige, brand distinctiveness, and symbolism."
)
)
kable(factor_table, caption = "Table: Factors and Their Interpretations Based on PCA (Varimax) Loadings", align = "l")
# Add factor ratings to the raw data
Data_with_scores <- cbind(Data_clean, PC1$scores)
# Let’s look at brand names (suppose the variable is called brandrec)
unique(Data_with_scores$brandrec)
# Compare average factor values by brand
library(dplyr)
#factor_by_brandrec <- Data_with_scores %>%
# group_by(brandrec) %>%
#summarise(across(RC1:RC9, mean, na.rm = TRUE))
#print(factor_by_brandrec)
# Combine factors and brands into one dataframe
Data_scores <- cbind(Data_clean["brandrec"], PC1$scores)
# Group by brand and calculate the average value for each factor
Factor_means <- Data_scores %>%
group_by(brandrec) %>%
summarise(across(starts_with("RC"), mean))
Factor_means
# Convert table to long format for ggplot
Factor_means_long <- Factor_means %>%
pivot_longer(cols = starts_with("RC"), names_to = "Factor", values_to = "Mean")
# Graph
ggplot(Factor_means_long, aes(x = Factor, y = Mean, fill = factor(brandrec))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Comparison of Brands by Factor Scores",
x = "Factor",
y = "Mean Score",
fill = "Brand") +
theme_minimal(base_size = 14)
# Сlear the database
rm(list=ls())
graphics.off()
#Readability in HTML report
#install.packages("DT")
library(DT)
# Clear the database
rm(list=ls())
#Install necessary packages. Delete # sign in order to download all of them.
#install.packages('rcompanion', repos = "https://cloud.r-project.org")
#install.packages('nortest', repos = "https://cloud.r-project.org")
#install.packages('corrplot', repos = "https://cloud.r-project.org")
#install.packages('olsrr', repos = "https://cloud.r-project.org")
#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
# Read the dataset directly from the provided URL
url <- "https://raw.githubusercontent.com/silinette/Case-Study-II/refs/heads/main/Data%20File_Case_Study_Factor%20Analysis_MD.csv"
Data <- read.csv(url, header = TRUE)
Data
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
# Select only variables needed for the analysis
Data <- Data %>%
select(
qd1, qd2, qd3, qd4, qd5, qd6, qd7, qd8, qd9,
qd10, qd11, qd12, qd13, qd14, qd15, qd16, qd17, qd18, qd19,
qd20, qd21, qd22, qd23, qd24, qd25, qd26, qd27, qd28, qd29,
qd30, qd31, qd32, qd33,
brandrec, wtp1, wtp2, wtp3, ri1, ri2
)
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values
# Remove all rows with missing values (listwise deletion)
Data_clean <- na.omit(Data)
# Сheck structure and dimensions of the new dataset
str(Data_clean)
dim(Data_clean)
sum(is.na(Data_clean))
#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)
# Descriptive statistics for 33 quality-related variables
# Descriptive statistics (mean, standard deviation, min, max, etc.) for the 33 quality-related survey items (qd1 to qd33) from the cleaned dataset. The results are rounded to 2 decimal places for readability.
# Normality test not included (norm = TRUE) because Likert-scale items are not truly continuous.
round(stat.desc(Data_clean[, paste0("qd", 1:33)]), digits = 3)
# Reshape only qd1 to qd33 into long format for visualization
data_long <- Data_clean %>%
select(paste0("qd", 1:33)) %>%  # Use only quality-related survey items
pivot_longer(everything(), names_to = "Item", values_to = "Response")  # Convert from wide to long format
# Order items numerically (qd1 to qd33) instead of alphabetically
data_long$Item <- factor(data_long$Item, levels = paste0("qd", 1:33))
# Preview reshaped data
head(data_long) # Show the first few rows of the long-format data to check the transformation
# Convert responses to ordered factor
data_long$Response <- factor(data_long$Response, levels = sort(unique(unlist(Data_clean)))) # Convert all numeric responses into an ordered factor to preserve Likert scale order in plots
# Plot response distributions for all items (directly in R Markdown)
ggplot(data_long, aes(x = Response)) + # Set x-axis to response values
geom_bar(fill = "violetred2") + # Create bar charts
facet_wrap(~ Item, scales = "free_y") + # One plot per item
labs(title = "Distributions", x = "Response", y = "Count") + # Title and axis labels
theme_minimal(base_size = 6)  # Clean theme
# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(as.matrix(raqMatrix))
# Run Bartlett's test of sphericity
bart_spher(Data_clean[, paste0("qd", 1:33)])
#KMO Criterion
kmo_test <- KMOS(Data_clean[, paste0("qd", 1:33)])
kmo_test$KMO           # Overall KMO measure
#Anti-Image Correlations
sort(kmo_test$MSA)     # Measure of sampling adequacy for each item
# Running PCA without rotation for scree plot analysis
PC0 <- principal(Data_clean[, paste0("qd", 1:33)],
rotate = "none")
PC0$values #Eigenvalues
# Scree plot of eigenvalues
plot(PC0$values, type = "b", main = "Scree Plot", xlab = "Factor Number", ylab = "Eigenvalue")
abline(h = 1) # Kaiser criterion line
# Extraction and displaying communalities
PC0_communalities <- data.frame(sort(PC0$communality))
PC0_communalities
# Principal Components Analysis (PCA) with Varimax Rotation
PC1 <- principal(
Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
rotate = "varimax",
nfactors = 8,
scores = TRUE
)
# Extract eigenvalues
EigenValue_pc1 <- PC1$values
EigenValue_pc1
# Sort and save communalities
PC1_communalities <- data.frame(sort(PC1$communality))
PC1_communalities
# Extraction eigenvalues from the PCA object
EigenValue <- PC1$values
# Number of items (qd1..qd33) used in factor analysis
n_items <- 31
# Calculating percentage of variance explained by each factor
Variance <- EigenValue / n_items * 100
# Cumulative variance explained
SumVariance <- cumsum(Variance)
# Building a summary table of eigenvalues and variance explained
Total_Variance_Explained <- cbind(
EigenValue = EigenValue[EigenValue > 0],
Variance = Variance[Variance > 0],
Total_Variance = SumVariance[Variance > 0]
)
# Viewing the table
Total_Variance_Explained
# Print factor loadings above 0.3, sorted by factor
# A threshold of 0.3 is commonly used to highlight meaningful factor-variable relationships
print(PC1$loadings, cutoff = 0.3, sort = TRUE)
library(knitr)
factor_table <- data.frame(
Factor = c("RC1", "RC2", "RC3", "RC4", "RC5", "RC6", "RC7", "RC8"),
Quality_Dimension = c(
"Performance",
"Serviceability",
"Aesthetics/Appearance",
"Durability",
"Ease of Use",
"Features/Versatility",
"Reliability/Flawlessness",
"Distinctiveness/Prestige"
),
Survey_Items = c(
"qd2, qd5, qd7, qd12, qd16",              # RC1
"qd9, qd14, qd19, qd21, qd24",             # RC2
"qd1, qd10, qd20, qd27",                   # RC3
"qd15, qd17, qd32",                        # RC4
"qd3, qd11, qd13, qd30",                   # RC5
"qd6, qd8, qd18, qd25",                    # RC6
"qd22, qd29, qd33",                        # RC7
"qd26, qd28, qd31"                         # RC8
),
Interpretation = c(
"Core performance and consistency in main functions.",
"Responsiveness and helpfulness of customer service.",
"Design, visual appeal, and overall look of the product.",
"Sturdiness and resistance to wear over time.",
"Easy to understand, navigate, and operate.",
"Advanced features and technical flexibility.",
"Lack of defects and high technical accuracy.",
"Prestige, brand distinctiveness, and symbolism."
)
)
kable(factor_table, caption = "Table: Factors and Their Interpretations Based on PCA (Varimax) Loadings", align = "l")
# Add factor ratings to the raw data
Data_with_scores <- cbind(Data_clean, PC1$scores)
# Let’s look at brand names (suppose the variable is called brandrec)
unique(Data_with_scores$brandrec)
# Compare average factor values by brand
library(dplyr)
#factor_by_brandrec <- Data_with_scores %>%
# group_by(brandrec) %>%
#summarise(across(RC1:RC9, mean, na.rm = TRUE))
#print(factor_by_brandrec)
# Combine factors and brands into one dataframe
Data_scores <- cbind(Data_clean["brandrec"], PC1$scores)
# Group by brand and calculate the average value for each factor
Factor_means <- Data_scores %>%
group_by(brandrec) %>%
summarise(across(starts_with("RC"), mean))
Factor_means
# Convert table to long format for ggplot
Factor_means_long <- Factor_means %>%
pivot_longer(cols = starts_with("RC"), names_to = "Factor", values_to = "Mean")
# Graph
ggplot(Factor_means_long, aes(x = Factor, y = Mean, fill = factor(brandrec))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Comparison of Brands by Factor Scores",
x = "Factor",
y = "Mean Score",
fill = "Brand") +
theme_minimal(base_size = 14)
# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(raqMatrix,
method = "color",
type = "upper",
tl.cex = 0.5,
number.cex = 0.5,
tl.col = "black",
mar = c(1,1,1,1),
cl.cex = 0.5)
# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(raqMatrix,
method = "color",
type = "upper",
col = colorRampPalette(c("white", "darkgreen"))(200),  # зелёная палитра
tl.cex = 0.5,
tl.col = "black",
number.cex = 0.5,
mar = c(1,1,1,1),
cl.cex = 0.5)
# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(raqMatrix,
method = "color",
type = "upper",
tl.cex = 0.5,
number.cex = 0.5,
tl.col = "black",
mar = c(1,1,1,1),
cl.cex = 0.5)
