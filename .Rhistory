#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
# Read the dataset directly from the provided URL
url <- "https://raw.githubusercontent.com/silinette/Case-Study-II/refs/heads/main/Data%20File_Case_Study_Factor%20Analysis_MD.csv"
Data <- read.csv(url, header = TRUE)
Data
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values
#Column names that we do not touch/Define variables we will keep even with NAs:
cols_keep_na <- c("noOfDefects", "defectsWarranty", "fRecall",
"def_sev1", "def_sev2", "didComplain",
"reactionComplain", "nextPurchase")
# Create column vector to check for clearances/Define variables to check for complete cases:
cols_check <- setdiff(names(Data), cols_keep_na)
# Delete the rows where there is NA in these columns (the rest are not touched):
Data_clean <- Data[complete.cases(Data[, cols_check]), ]
Data_clean
#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)
# Descriptive statistics for 33 quality-related variables
round(stat.desc(Data_clean[, paste0("qd", 1:33)]), digits = 2)
# Descriptive statistics (mean, standard deviation, min, max, etc.) for the 33 quality-related survey items (qd1 to qd33) from the cleaned dataset. The results are rounded to 2 decimal places for readability.
# Normality test not included (norm = TRUE) because Likert-scale items are not truly continuous.
# Reshape the data into long format for visualization
data_long <- Data_clean %>%
pivot_longer(everything(), names_to = "Item", values_to = "Response") # Convert the wide-format data frame into long format, where each row represents one response to one item.
# Preview reshaped data
head(data_long) # Show the first few rows of the long-format data to check the transformation
# Convert responses to ordered factor
data_long$Response <- factor(data_long$Response, levels = sort(unique(unlist(Data_clean)))) # Convert all numeric responses into an ordered factor to preserve Likert scale order in plots
# Save barplots to PDF for visual inspection
pdf("~/Desktop/Distributions_by_Item.pdf", width = 30, height = 30)
# Plot response distributions for all items
ggplot(data_long, aes(x = Response)) + # Set x-axis to response values
geom_bar(fill = "steelblue") + # Create blue bar charts
facet_wrap(~ Item, scales = "free_y") + # Create separate plot for each item
labs(title = "Distributions", x = "Response", y = "Count") + # Add title and axis labels
theme_minimal()  # Use a clean and minimal theme for the plot
dev.off()
# Sorry, not enough space in R markdown :(
# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(as.matrix(raqMatrix))
# Run Bartlett's test of sphericity
bart_spher(Data_clean[, paste0("qd", 1:33)])
#KMO Criterion
kmo_test <- KMOS(Data_clean[, paste0("qd", 1:33)])
kmo_test$KMO           # Overall KMO measure
#Anti-Image Correlations
sort(kmo_test$MSA)     # Measure of sampling adequacy for each item
# Running PCA without rotation for scree plot analysis
PC0 <- principal(Data_clean[, paste0("qd", 1:33)],
rotate = "none")
PC0$values #Eigenvalues
# Scree plot of eigenvalues
plot(PC0$values, type = "b", main = "Scree Plot", xlab = "Factor Number", ylab = "Eigenvalue")
abline(h = 1) # Kaiser criterion line
# Extraction and displaying communalities
PC0_communalities <- data.frame(sort(PC0$communality))
PC0_communalities
# Principal Components Analysis (PCA) with Varimax Rotation
PC1 <- principal(
Data_clean[, paste0("qd", 1:33)],  # use cleaned data and qd1..qd33 columns
rotate = "varimax",
nfactors = 9,
scores = TRUE
)
# Extract eigenvalues
EigenValue_pc1 <- PC1$values
EigenValue_pc1
# Sort and save communalities
PC1_communalities <- data.frame(sort(PC1$communality))
PC1_communalities
# Extraction eigenvalues from the PCA object
EigenValue <- PC1$values
# Number of items (qd1..qd33) used in factor analysis
n_items <- 33
# Calculating percentage of variance explained by each factor
Variance <- EigenValue / n_items * 100
# Cumulative variance explained
SumVariance <- cumsum(Variance)
# Building a summary table of eigenvalues and variance explained
Total_Variance_Explained <- cbind(
EigenValue = EigenValue[EigenValue > 0],
Variance = Variance[Variance > 0],
Total_Variance = SumVariance[Variance > 0]
)
# Viewing the table
Total_Variance_Explained
# Print factor loadings above 0.3, sorted by factor
# A threshold of 0.3 is commonly used to highlight meaningful factor-variable relationships
print(PC1$loadings, cutoff = 0.3, sort = TRUE)
library(knitr)
factor_table <- data.frame(
Factor = c("RC1", "RC2", "RC3", "RC4", "RC5", "RC6", "RC7", "RC8", "RC9"),
Quality_Dimension = c(
"Performance",
"Features/Versatility",
"Reliability/Flawlessness",
"Durability",
"Ease of Use",
"Serviceability",
"Aesthetics/Appearance",
"Distinctiveness/Prestige",
"Conformance (Build Quality)"
),
Survey_Items = c(
"qd2, qd5, qd7, qd12, qd16",
"qd6, qd8, qd18, qd25",
"qd22, qd26, qd29, qd33",
"qd15, qd17, qd28, qd31",
"qd3, qd11, qd13, qd30",
"qd9, qd14, qd19, qd21, qd24",
"qd1, qd10, qd20",
"qd4, qd23",
"qd27, qd32"
),
Interpretation = c(
"Core performance and operational consistency.",
"Innovative and exciting features beyond core functionality.",
"Freedom from defects and malfunctions.",
"Longevity and resistance to wear and heavy usage.",
"User-friendliness and ease of operation.",
"Quality and responsiveness of customer service.",
"Visual appeal and design of the product.",
"Uniqueness and symbolic value of the smartphone.",
"Fit, finish, and use of quality materials."
)
)
kable(factor_table, caption = "Table: Factors and Their Interpretations Based on Survey Items", align = "l")
# Добавим факторные оценки к исходным данным
Data_with_scores <- cbind(Data_clean, PC1$scores)
# Посмотрим названия брендов (предположим, переменная называется brand)
unique(Data_with_scores$brand)  # Проверка
# Сравним средние значения факторов по брендам
library(dplyr)
factor_by_brand <- Data_with_scores %>%
group_by(brand) %>%
summarise(across(RC1:RC9, mean, na.rm = TRUE))
print(factor_by_brand)
# Объединяем факторы и бренды в один датафрейм
Data_scores <- cbind(Data_clean["brand"], PC1$scores)
# Группируем по бренду и считаем среднее значение по каждому фактору
Factor_means <- Data_scores %>%
group_by(brand) %>%
summarise(across(starts_with("RC"), mean))
Factor_means
# Преобразуем таблицу в длинный формат для ggplot
Factor_means_long <- Factor_means %>%
pivot_longer(cols = starts_with("RC"), names_to = "Factor", values_to = "Mean")
# Строим график
ggplot(Factor_means_long, aes(x = Factor, y = Mean, fill = factor(brand))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Comparison of Brands by Factor Scores",
x = "Factor",
y = "Mean Score",
fill = "Brand") +
theme_minimal(base_size = 14)
unique(Data$brand)
table(Data$brand)
unique(Data_clean$brand)
table(Data_clean$brand)
# Calculate mean factor scores for each respondent
df$Aesthetics     <- rowMeans(df[ , c("qd1", "qd10", "qd20", "qd27") ], na.rm = TRUE)
library(dplyr)
# Calculate mean factor score per brand
brand_means <- df %>%
group_by(brand) %>%
summarise(
Aesthetics   = mean(Aesthetics, na.rm = TRUE),
Durability   = mean(Durability, na.rm = TRUE),
Ease_of_Use  = mean(Ease_of_Use, na.rm = TRUE),
Features     = mean(Features, na.rm = TRUE),
Performance  = mean(Performance, na.rm = TRUE),
Reliability  = mean(Reliability, na.rm = TRUE),
Serviceability = mean(Serviceability, na.rm = TRUE),
Prestige     = mean(Prestige, na.rm = TRUE)
)
# Necessary step
df <- Data_with_scores
# Добавим средние по факторам (по оригинальным пунктам опроса)
df$Aesthetics     <- rowMeans(df[ , c("qd1", "qd10", "qd20", "qd27") ], na.rm = TRUE)
df$Durability     <- rowMeans(df[ , c("qd15", "qd17", "qd31", "qd32") ], na.rm = TRUE)
df$Ease_of_Use    <- rowMeans(df[ , c("qd3", "qd11", "qd13", "qd30") ], na.rm = TRUE)
df$Features       <- rowMeans(df[ , c("qd6", "qd8", "qd18", "qd25") ], na.rm = TRUE)
df$Performance    <- rowMeans(df[ , c("qd2", "qd5", "qd7", "qd12", "qd16") ], na.rm = TRUE)
df$Reliability    <- rowMeans(df[ , c("qd22", "qd26", "qd28", "qd29", "qd33") ], na.rm = TRUE)
df$Serviceability <- rowMeans(df[ , c("qd9", "qd14", "qd19", "qd21", "qd24") ], na.rm = TRUE)
df$Prestige       <- rowMeans(df[ , c("qd4", "qd23") ], na.rm = TRUE)
# Группируем по брендам и считаем средние значения
brand_means <- df %>%
group_by(brand) %>%
summarise(
Aesthetics     = mean(Aesthetics, na.rm = TRUE),
Durability     = mean(Durability, na.rm = TRUE),
Ease_of_Use    = mean(Ease_of_Use, na.rm = TRUE),
Features       = mean(Features, na.rm = TRUE),
Performance    = mean(Performance, na.rm = TRUE),
Reliability    = mean(Reliability, na.rm = TRUE),
Serviceability = mean(Serviceability, na.rm = TRUE),
Prestige       = mean(Prestige, na.rm = TRUE)
)
print(brand_means)
# Necessary step
df <- Data_with_scores
# Add average by factors (based on original survey points)
df$Aesthetics     <- rowMeans(df[ , c("qd1", "qd10", "qd20", "qd27") ], na.rm = TRUE)
df$Durability     <- rowMeans(df[ , c("qd15", "qd17", "qd31", "qd32") ], na.rm = TRUE)
df$Ease_of_Use    <- rowMeans(df[ , c("qd3", "qd11", "qd13", "qd30") ], na.rm = TRUE)
df$Features       <- rowMeans(df[ , c("qd6", "qd8", "qd18", "qd25") ], na.rm = TRUE)
df$Performance    <- rowMeans(df[ , c("qd2", "qd5", "qd7", "qd12", "qd16") ], na.rm = TRUE)
df$Reliability    <- rowMeans(df[ , c("qd22", "qd26", "qd28", "qd29", "qd33") ], na.rm = TRUE)
df$Serviceability <- rowMeans(df[ , c("qd9", "qd14", "qd19", "qd21", "qd24") ], na.rm = TRUE)
df$Prestige       <- rowMeans(df[ , c("qd4", "qd23") ], na.rm = TRUE)
# Group by brands and count the means
brand_means <- df %>%
group_by(brand) %>%
summarise(
Aesthetics     = mean(Aesthetics, na.rm = TRUE),
Durability     = mean(Durability, na.rm = TRUE),
Ease_of_Use    = mean(Ease_of_Use, na.rm = TRUE),
Features       = mean(Features, na.rm = TRUE),
Performance    = mean(Performance, na.rm = TRUE),
Reliability    = mean(Reliability, na.rm = TRUE),
Serviceability = mean(Serviceability, na.rm = TRUE),
Prestige       = mean(Prestige, na.rm = TRUE)
)
# Viewing the table
print(brand_means)
# Сlear the database
rm(list=ls())
graphics.off()
#Readability in HTML report
#install.packages("DT")
library(DT)
# Clear the database
rm(list=ls())
#Install necessary packages. Delete # sign in order to download all of them.
#install.packages('rcompanion', repos = "https://cloud.r-project.org")
#install.packages('nortest', repos = "https://cloud.r-project.org")
#install.packages('corrplot', repos = "https://cloud.r-project.org")
#install.packages('olsrr', repos = "https://cloud.r-project.org")
#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
# Read the dataset directly from the provided URL
url <- "https://raw.githubusercontent.com/silinette/Case-Study-II/refs/heads/main/Data%20File_Case_Study_Factor%20Analysis_MD.csv"
Data <- read.csv(url, header = TRUE)
Data
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values
#Column names that we do not touch/Define variables we will keep even with NAs:
cols_keep_na <- c("noOfDefects", "defectsWarranty", "fRecall",
"def_sev1", "def_sev2", "didComplain",
"reactionComplain", "nextPurchase")
# Create column vector to check for clearances/Define variables to check for complete cases:
cols_check <- setdiff(names(Data), cols_keep_na)
# Delete the rows where there is NA in these columns (the rest are not touched):
# Filter for Samsung only (brandrec == 2) and apply listwise deletion
Data_clean <- Data %>%
filter(brandrec == 2) %>%                # keep only Samsung
filter(complete.cases(across(all_of(cols_check))))  # listwise deletion
#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)
# Descriptive statistics for 33 quality-related variables
round(stat.desc(Data_clean[, paste0("qd", 1:33)]), digits = 2)
# Descriptive statistics (mean, standard deviation, min, max, etc.) for the 33 quality-related survey items (qd1 to qd33) from the cleaned dataset. The results are rounded to 2 decimal places for readability.
# Normality test not included (norm = TRUE) because Likert-scale items are not truly continuous.
# Reshape the data into long format for visualization
data_long <- Data_clean %>%
pivot_longer(everything(), names_to = "Item", values_to = "Response") # Convert the wide-format data frame into long format, where each row represents one response to one item.
# Preview reshaped data
head(data_long) # Show the first few rows of the long-format data to check the transformation
# Convert responses to ordered factor
data_long$Response <- factor(data_long$Response, levels = sort(unique(unlist(Data_clean)))) # Convert all numeric responses into an ordered factor to preserve Likert scale order in plots
# Save barplots to PDF for visual inspection
pdf("~/Desktop/Distributions_by_Item.pdf", width = 30, height = 30)
# Plot response distributions for all items
ggplot(data_long, aes(x = Response)) + # Set x-axis to response values
geom_bar(fill = "steelblue") + # Create blue bar charts
facet_wrap(~ Item, scales = "free_y") + # Create separate plot for each item
labs(title = "Distributions", x = "Response", y = "Count") + # Add title and axis labels
theme_minimal()  # Use a clean and minimal theme for the plot
dev.off()
# Sorry, not enough space in R markdown :(
# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(as.matrix(raqMatrix))
# Run Bartlett's test of sphericity
bart_spher(Data_clean[, paste0("qd", 1:33)])
#KMO Criterion
kmo_test <- KMOS(Data_clean[, paste0("qd", 1:33)])
kmo_test$KMO           # Overall KMO measure
#Anti-Image Correlations
sort(kmo_test$MSA)     # Measure of sampling adequacy for each item
# Running PCA without rotation for scree plot analysis
PC0 <- principal(Data_clean[, paste0("qd", 1:33)],
rotate = "none")
PC0$values #Eigenvalues
# Scree plot of eigenvalues
plot(PC0$values, type = "b", main = "Scree Plot", xlab = "Factor Number", ylab = "Eigenvalue")
abline(h = 1) # Kaiser criterion line
# Extraction and displaying communalities
PC0_communalities <- data.frame(sort(PC0$communality))
PC0_communalities
# Principal Components Analysis (PCA) with Varimax Rotation
PC1 <- principal(
Data_clean[, paste0("qd", 1:33)],  # use cleaned data and qd1..qd33 columns
rotate = "varimax",
nfactors = 9,
scores = TRUE
)
# Extract eigenvalues
EigenValue_pc1 <- PC1$values
EigenValue_pc1
# Sort and save communalities
PC1_communalities <- data.frame(sort(PC1$communality))
PC1_communalities
# Extraction eigenvalues from the PCA object
EigenValue <- PC1$values
# Number of items (qd1..qd33) used in factor analysis
n_items <- 33
# Calculating percentage of variance explained by each factor
Variance <- EigenValue / n_items * 100
# Cumulative variance explained
SumVariance <- cumsum(Variance)
# Building a summary table of eigenvalues and variance explained
Total_Variance_Explained <- cbind(
EigenValue = EigenValue[EigenValue > 0],
Variance = Variance[Variance > 0],
Total_Variance = SumVariance[Variance > 0]
)
# Viewing the table
Total_Variance_Explained
# Print factor loadings above 0.3, sorted by factor
# A threshold of 0.3 is commonly used to highlight meaningful factor-variable relationships
print(PC1$loadings, cutoff = 0.3, sort = TRUE)
library(knitr)
factor_table <- data.frame(
Factor = c("RC1", "RC2", "RC3", "RC4", "RC5", "RC6", "RC7", "RC8", "RC9"),
Quality_Dimension = c(
"Performance",
"Features/Versatility",
"Reliability/Flawlessness",
"Durability",
"Ease of Use",
"Serviceability",
"Aesthetics/Appearance",
"Distinctiveness/Prestige",
"Conformance (Build Quality)"
),
Survey_Items = c(
"qd2, qd5, qd7, qd12, qd16",
"qd6, qd8, qd18, qd25",
"qd22, qd26, qd29, qd33",
"qd15, qd17, qd28, qd31",
"qd3, qd11, qd13, qd30",
"qd9, qd14, qd19, qd21, qd24",
"qd1, qd10, qd20",
"qd4, qd23",
"qd27, qd32"
),
Interpretation = c(
"Core performance and operational consistency.",
"Innovative and exciting features beyond core functionality.",
"Freedom from defects and malfunctions.",
"Longevity and resistance to wear and heavy usage.",
"User-friendliness and ease of operation.",
"Quality and responsiveness of customer service.",
"Visual appeal and design of the product.",
"Uniqueness and symbolic value of the smartphone.",
"Fit, finish, and use of quality materials."
)
)
kable(factor_table, caption = "Table: Factors and Their Interpretations Based on Survey Items", align = "l")
# Add factor ratings to the raw data
Data_with_scores <- cbind(Data_clean, PC1$scores)
# Let’s look at the brand names (the variable is called brand)
unique(Data_with_scores$brand)  # Checking
# Compare average factor values by brand
library(dplyr)
factor_by_brand <- Data_with_scores %>%
group_by(brand) %>%
summarise(across(RC1:RC9, mean, na.rm = TRUE))
print(factor_by_brand)
# Combine factors and brands into one dataframe
Data_scores <- cbind(Data_clean["brandrec"], PC1$scores)
# Group by brand and calculate the average value for each factor
Factor_means <- Data_scores %>%
group_by(brand) %>%
summarise(across(starts_with("RC"), mean))
# Сlear the database
rm(list=ls())
graphics.off()
#Readability in HTML report
#install.packages("DT")
library(DT)
# Clear the database
rm(list=ls())
#Install necessary packages. Delete # sign in order to download all of them.
#install.packages('rcompanion', repos = "https://cloud.r-project.org")
#install.packages('nortest', repos = "https://cloud.r-project.org")
#install.packages('corrplot', repos = "https://cloud.r-project.org")
#install.packages('olsrr', repos = "https://cloud.r-project.org")
#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
# Read the dataset directly from the provided URL
url <- "https://raw.githubusercontent.com/silinette/Case-Study-II/refs/heads/main/Data%20File_Case_Study_Factor%20Analysis_MD.csv"
Data <- read.csv(url, header = TRUE)
Data
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values
#Column names that we do not touch/Define variables we will keep even with NAs:
cols_keep_na <- c("noOfDefects", "defectsWarranty", "fRecall",
"def_sev1", "def_sev2", "didComplain",
"reactionComplain", "nextPurchase")
# Create column vector to check for clearances/Define variables to check for complete cases:
cols_check <- setdiff(names(Data), cols_keep_na)
# Delete the rows where there is NA in these columns (the rest are not touched):
Data_clean <- Data[complete.cases(Data[, cols_check]), ]
Data_clean
#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values
#Column names that we do not touch/Define variables we will keep even with NAs:
cols_keep_na <- c("noOfDefects", "defectsWarranty", "fRecall",
"def_sev1", "def_sev2", "didComplain",
"reactionComplain", "nextPurchase")
# Create column vector to check for clearances/Define variables to check for complete cases:
cols_check <- setdiff(names(Data), cols_keep_na)
# Delete the rows where there is NA in these columns (the rest are not touched):
# Filter for Samsung only (brandrec == 2) and apply listwise deletion
Data_clean <- Data %>%
filter(brandrec == 2) %>%                # keep only Samsung
filter(complete.cases(across(all_of(cols_check))))  # listwise deletion
#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
```
```
#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)
Data_clean
