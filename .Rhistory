Metric = c("SS Loadings", "Proportion of Variance", "Cumulative Variance"),
`RC2 (PCA)` = c(4.673, 0.151, 0.151),
`PA2 (PAF)` = c(4.569, 0.147, 0.147),
`RC1 (PCA)` = c(3.908, 0.126, 0.277),
`PA1 (PAF)` = c(3.577, 0.115, 0.263),
`RC5 (PCA)` = c(3.492, 0.113, 0.389),
`PA5 (PAF)` = c(3.404, 0.110, 0.373),
`RC4 (PCA)` = c(3.297, 0.106, 0.496),
`PA4 (PAF)` = c(3.121, 0.101, 0.473),
`RC7 (PCA)` = c(3.020, 0.097, 0.593),
`PA3 (PAF)` = c(2.781, 0.090, 0.563),
`RC6 (PCA)` = c(2.856, 0.092, 0.685),
`PA7 (PAF)` = c(2.675, 0.086, 0.649),
`RC3 (PCA)` = c(2.492, 0.080, 0.766),
`PA6 (PAF)` = c(2.324, 0.075, 0.724),
`RC8 (PCA)` = c(2.337, 0.075, 0.841),
`PA8 (PAF)` = c(1.935, 0.062, 0.787)
)
# –í—ã–≤–æ–¥ –≤ HTML-—Å—Ç–∏–ª–µ (–≤ –æ—Ç—á—ë—Ç–µ –±—É–¥–µ—Ç –∞–∫–∫—É—Ä–∞—Ç–Ω–æ)
knitr::kable(
comparison_df,
caption = "Side-by-Side Comparison of Factor Metrics from PCA and PAF (Varimax)",
align = c("l", rep("c", ncol(comparison_df) - 1))
)
factor_table <- data.frame(
Factor = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8"),
Quality_Dimension = c(
"Serviceability",
"Performance",
"Reliability/Flawlessness",
"Ease of Use",
"Aesthetics/Appearance",
"Features/Versatility",
"Durability",
"Distinctiveness/Prestige"
),
Survey_Items = c(
"qd2, qd5, qd7, qd12, qd16",               # PA1
"qd9, qd14, qd19, qd21, qd24",             # PA2
"qd22, qd29, qd33",                        # PA3
"qd6, qd8, qd18, qd25",                    # PA4
"qd3, qd11, qd13, qd30",                   # PA5
"qd15, qd17, qd32",                        # PA6
"qd1, qd10, qd20, qd27",                   # PA7
"qd26, qd28, qd31"                         # PA8
)
)
kable(factor_table, caption = "Table: Factors and Their Interpretations Based on PAF (Varimax) Loadings", align = "l")
library(knitr)
factor_table <- data.frame(
Factor = c("RC1", "RC2", "RC3", "RC4", "RC5", "RC6", "RC7", "RC8"),
Quality_Dimension = c(
"Performance",
"Serviceability",
"Aesthetics/Appearance",
"Durability",
"Ease of Use",
"Features/Versatility",
"Reliability/Flawlessness",
"Distinctiveness/Prestige"
),
Survey_Items = c(
"qd1, qd10, qd20, qd27",                   # RC1
"qd9, qd14, qd19, qd21, qd24",             # RC2
"qd22, qd29, qd33",                        # RC3
"qd15, qd17, qd32",                        # RC4
"qd3, qd11, qd13, qd30",                   # RC5
"qd6, qd8, qd18, qd25",                    # RC6
"qd2, qd5, qd7, qd12, qd16",               # RC7
"qd26, qd28, qd31"                         # RC8
)
)
kable(factor_table, caption = "Table: Factors and Their Interpretations Based on PCA (Varimax) Loadings", align = "l")
# Run Principal Axis Factoring with oblique (Promax) rotation
PAF_promax <- fa(
Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
nfactors = 8,
rotate = "promax",
fm = "pa",
scores = TRUE
)
# Extract eigenvalues
EigenValue_PAF_promax <- PAF_promax$e.values
EigenValue_PAF_promax
# Communalities
PAF_promax_communalities <- data.frame(sort(PAF_promax$communality))
PAF_promax_communalities
print(PCA1$loadings, cutoff = 0.3, sort = TRUE)
#Readability in HTML report
#install.packages("DT")
library(DT)
# Clear the database
rm(list=ls())
#Install necessary packages. Delete # sign in order to download all of them.
#install.packages('rcompanion', repos = "https://cloud.r-project.org")
#install.packages('nortest', repos = "https://cloud.r-project.org")
#install.packages('corrplot', repos = "https://cloud.r-project.org")
#install.packages('olsrr', repos = "https://cloud.r-project.org")
#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
#install.packages("GPArotation", repos = "https://cloud.r-project.org")
# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
#Readability in HTML report
#install.packages("DT")
library(DT)
# Clear the database
rm(list=ls())
#Install necessary packages. Delete # sign in order to download all of them.
#install.packages('rcompanion', repos = "https://cloud.r-project.org")
#install.packages('nortest', repos = "https://cloud.r-project.org")
#install.packages('corrplot', repos = "https://cloud.r-project.org")
#install.packages('olsrr', repos = "https://cloud.r-project.org")
#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
#install.packages("GPArotation", repos = "https://cloud.r-project.org")
# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
library(GPArotation)
# Run Principal Axis Factoring with oblique (Promax) rotation
PAF_promax <- fa(
Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
nfactors = 8,
rotate = "promax",
fm = "pa",
scores = TRUE
)
# –°lear the database
rm(list=ls())
graphics.off()
#Readability in HTML report
#install.packages("DT")
library(DT)
# Clear the database
rm(list=ls())
#Install necessary packages. Delete # sign in order to download all of them.
#install.packages('rcompanion', repos = "https://cloud.r-project.org")
#install.packages('nortest', repos = "https://cloud.r-project.org")
#install.packages('corrplot', repos = "https://cloud.r-project.org")
#install.packages('olsrr', repos = "https://cloud.r-project.org")
#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
#install.packages("GPArotation", repos = "https://cloud.r-project.org")
# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
library(GPArotation)
# Read the dataset directly from the provided URL
url <- "https://raw.githubusercontent.com/silinette/Case-Study-II/refs/heads/main/Data%20File_Case_Study_Factor%20Analysis_MD.csv"
Data <- read.csv(url, header = TRUE)
Data
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
# Select only variables needed for the analysis
Data <- Data %>%
select(
qd1, qd2, qd3, qd4, qd5, qd6, qd7, qd8, qd9,
qd10, qd11, qd12, qd13, qd14, qd15, qd16, qd17, qd18, qd19,
qd20, qd21, qd22, qd23, qd24, qd25, qd26, qd27, qd28, qd29,
qd30, qd31, qd32, qd33,
brandrec, wtp1, wtp2, wtp3, ri1, ri2
)
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values
# Remove all rows with missing values (listwise deletion)
Data_clean <- na.omit(Data)
# –°heck structure and dimensions of the new dataset
str(Data_clean)
dim(Data_clean)
sum(is.na(Data_clean))
#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)
# Descriptive statistics for 33 quality-related variables
# Descriptive statistics (mean, standard deviation, min, max, etc.) for the 33 quality-related survey items (qd1 to qd33) from the cleaned dataset. The results are rounded to 2 decimal places for readability.
# Normality test not included (norm = TRUE) because Likert-scale items are not truly continuous.
round(stat.desc(Data_clean[, paste0("qd", 1:33)]), digits = 3)
# Reshape only qd1 to qd33 into long format for visualization
data_long <- Data_clean %>%
select(paste0("qd", 1:33)) %>%  # Use only quality-related survey items
pivot_longer(everything(), names_to = "Item", values_to = "Response")  # Convert from wide to long format
# Order items numerically (qd1 to qd33) instead of alphabetically
data_long$Item <- factor(data_long$Item, levels = paste0("qd", 1:33))
# Preview reshaped data
head(data_long) # Show the first few rows of the long-format data to check the transformation
# Convert responses to ordered factor
data_long$Response <- factor(data_long$Response, levels = sort(unique(unlist(Data_clean)))) # Convert all numeric responses into an ordered factor to preserve Likert scale order in plots
# Plot response distributions for all items (directly in R Markdown)
ggplot(data_long, aes(x = Response)) + # Set x-axis to response values
geom_bar(fill = "violetred2") + # Create bar charts
facet_wrap(~ Item, scales = "free_y") + # One plot per item
labs(title = "Distributions", x = "Response", y = "Count") + # Title and axis labels
theme_minimal(base_size = 6)  # Clean theme
# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(raqMatrix,
method = "color",
type = "upper",
tl.cex = 0.5,
number.cex = 0.5,
tl.col = "black",
mar = c(1,1,1,1),
cl.cex = 0.5)
# Run Bartlett's test of sphericity
bart_spher(Data_clean[, paste0("qd", 1:33)])
#KMO Criterion
kmo_test <- KMOS(Data_clean[, paste0("qd", 1:33)])
kmo_test$KMO           # Overall KMO measure
#Anti-Image Correlations
sort(kmo_test$MSA)     # Measure of sampling adequacy for each item
# Running PCA without rotation for scree plot analysis
PC0 <- principal(
Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
rotate = "none"
)
PC0$values #Eigenvalues
# Scree plot of eigenvalues
plot(PC0$values, type = "b", main = "Scree Plot", xlab = "Factor Number", ylab = "Eigenvalue")
abline(h = 1) # Kaiser criterion line
# Extraction and displaying communalities
PC0_communalities <- data.frame(sort(PC0$communality))
PC0_communalities
# Principal Components Analysis (PCA) with Varimax Rotation
PCA1 <- principal(
Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
rotate = "varimax",
nfactors = 8,
scores = TRUE
)
# Extract eigenvalues
EigenValue_PCA1 <- PCA1$values
EigenValue_PCA1
# Sort and save communalities
PCA1_communalities <- data.frame(sort(PCA1$communality))
PCA1_communalities
### **Variance Explained Table for PCA Components**
# Extraction eigenvalues from the PCA object
EigenValue <- PCA1$values
# Number of items (qd1..qd33) used in factor analysis
n_items <- 31
# Calculating percentage of variance explained by each factor
Variance <- EigenValue / n_items * 100
# Cumulative variance explained
SumVariance <- cumsum(Variance)
# Building a summary table of eigenvalues and variance explained
Total_Variance_Explained <- cbind(
EigenValue = EigenValue[EigenValue > 0],
Variance = Variance[Variance > 0],
Total_Variance = SumVariance[Variance > 0]
)
# Viewing the table
Total_Variance_Explained
# Print factor loadings above 0.3, sorted by factor
# A threshold of 0.3 is commonly used to highlight meaningful factor-variable relationships
print(PCA1$loadings, cutoff = 0.3, sort = TRUE)
library(knitr)
factor_table <- data.frame(
Factor = c("RC1", "RC2", "RC3", "RC4", "RC5", "RC6", "RC7", "RC8"),
Quality_Dimension = c(
"Performance",
"Serviceability",
"Aesthetics/Appearance",
"Durability",
"Ease of Use",
"Features/Versatility",
"Reliability/Flawlessness",
"Distinctiveness/Prestige"
),
Survey_Items = c(
"qd1, qd10, qd20, qd27",                   # RC1
"qd9, qd14, qd19, qd21, qd24",             # RC2
"qd22, qd29, qd33",                        # RC3
"qd15, qd17, qd32",                        # RC4
"qd3, qd11, qd13, qd30",                   # RC5
"qd6, qd8, qd18, qd25",                    # RC6
"qd2, qd5, qd7, qd12, qd16",               # RC7
"qd26, qd28, qd31"                         # RC8
)
)
kable(factor_table, caption = "Table: Factors and Their Interpretations Based on PCA (Varimax) Loadings", align = "l")
# Run Principal Axis Factoring (PAF) with Varimax rotation
PAF1 <- fa(
Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
nfactors = 8,
rotate = "varimax",
fm = "pa",           # –º–µ—Ç–æ–¥ extraction = Principal Axis
scores = TRUE
)
# Extract eigenvalues
EigenValue_PAF1 <- PAF1$e.values
EigenValue_PAF1
# Sort and save communalities
PAF1_communalities <- data.frame(sort(PAF1$communality))
PAF1_communalities
# Print factor loadings above 0.3, sorted by factor
# A threshold of 0.3 is commonly used to highlight meaningful factor-variable relationships
print(PAF1$loadings, cutoff = 0.3, sort = TRUE)
factor_table <- data.frame(
Factor = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8"),
Quality_Dimension = c(
"Serviceability",
"Performance",
"Reliability/Flawlessness",
"Ease of Use",
"Aesthetics/Appearance",
"Features/Versatility",
"Durability",
"Distinctiveness/Prestige"
),
Survey_Items = c(
"qd2, qd5, qd7, qd12, qd16",               # PA1
"qd9, qd14, qd19, qd21, qd24",             # PA2
"qd22, qd29, qd33",                        # PA3
"qd6, qd8, qd18, qd25",                    # PA4
"qd3, qd11, qd13, qd30",                   # PA5
"qd15, qd17, qd32",                        # PA6
"qd1, qd10, qd20, qd27",                   # PA7
"qd26, qd28, qd31"                         # PA8
)
)
kable(factor_table, caption = "Table: Factors and Their Interpretations Based on PAF (Varimax) Loadings", align = "l")
#PCA
Eigen_PCA <- PCA1$values
Variance_PCA <- Eigen_PCA / 31 * 100
CumVar_PCA <- cumsum(Variance_PCA)
#PAF
Eigen_PAF <- PAF1$e.values
Variance_PAF <- Eigen_PAF / 31 * 100
CumVar_PAF <- cumsum(Variance_PAF)
# –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–∞—Ä—ã —Ä—è–¥–æ–º
Comparison_Table <- data.frame(
Factor = 1:length(Eigen_PCA),
Eigen_PCA = round(Eigen_PCA, 3),
Eigen_PAF = round(Eigen_PAF, 3),
Variance_PCA = round(Variance_PCA, 2),
Variance_PAF = round(Variance_PAF, 2),
Cumulative_PCA = round(CumVar_PCA, 2),
Cumulative_PAF = round(CumVar_PAF, 2)
)
# –ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏–º
kable(Comparison_Table, caption = "Side-by-Side Comparison of PCA and PAF (Varimax)")
# –≠—Å—Ç–µ—Ç–∏—á–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è PCA –∏ PAF —Ñ–∞–∫—Ç–æ—Ä–æ–≤
comparison_df <- data.frame(
Metric = c("SS Loadings", "Proportion of Variance", "Cumulative Variance"),
`RC2 (PCA)` = c(4.673, 0.151, 0.151),
`PA2 (PAF)` = c(4.569, 0.147, 0.147),
`RC1 (PCA)` = c(3.908, 0.126, 0.277),
`PA1 (PAF)` = c(3.577, 0.115, 0.263),
`RC5 (PCA)` = c(3.492, 0.113, 0.389),
`PA5 (PAF)` = c(3.404, 0.110, 0.373),
`RC4 (PCA)` = c(3.297, 0.106, 0.496),
`PA4 (PAF)` = c(3.121, 0.101, 0.473),
`RC7 (PCA)` = c(3.020, 0.097, 0.593),
`PA3 (PAF)` = c(2.781, 0.090, 0.563),
`RC6 (PCA)` = c(2.856, 0.092, 0.685),
`PA7 (PAF)` = c(2.675, 0.086, 0.649),
`RC3 (PCA)` = c(2.492, 0.080, 0.766),
`PA6 (PAF)` = c(2.324, 0.075, 0.724),
`RC8 (PCA)` = c(2.337, 0.075, 0.841),
`PA8 (PAF)` = c(1.935, 0.062, 0.787)
)
# –í—ã–≤–æ–¥ –≤ HTML-—Å—Ç–∏–ª–µ (–≤ –æ—Ç—á—ë—Ç–µ –±—É–¥–µ—Ç –∞–∫–∫—É—Ä–∞—Ç–Ω–æ)
knitr::kable(
comparison_df,
caption = "Side-by-Side Comparison of Factor Metrics from PCA and PAF (Varimax)",
align = c("l", rep("c", ncol(comparison_df) - 1))
)
# Run Principal Axis Factoring with oblique (Promax) rotation
PAF_promax <- fa(
Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
nfactors = 8,
rotate = "promax",
fm = "pa",
scores = TRUE
)
# Extract eigenvalues
EigenValue_PAF_promax <- PAF_promax$e.values
EigenValue_PAF_promax
# Communalities
PAF_promax_communalities <- data.frame(sort(PAF_promax$communality))
PAF_promax_communalities
print(PCA1$loadings, cutoff = 0.3, sort = TRUE)
# Add factor ratings to the raw data
Data_with_scores <- cbind(Data_clean, PCA1$scores)
# Let‚Äôs look at brand names (suppose the variable is called brandrec)
unique(Data_with_scores$brandrec)
# Compare average factor values by brand
library(dplyr)
#factor_by_brandrec <- Data_with_scores %>%
# group_by(brandrec) %>%
#summarise(across(RC1:RC9, mean, na.rm = TRUE))
#print(factor_by_brandrec)
# Combine factors and brands into one dataframe
Data_scores <- cbind(Data_clean["brandrec"], PCA1$scores)
# Group by brand and calculate the average value for each factor
Factor_means <- Data_scores %>%
group_by(brandrec) %>%
summarise(across(starts_with("RC"), mean))
Factor_means
# Convert table to long format for ggplot
Factor_means_long <- Factor_means %>%
pivot_longer(cols = starts_with("RC"), names_to = "Factor", values_to = "Mean")
# Graph
ggplot(Factor_means_long, aes(x = Factor, y = Mean, fill = factor(brandrec))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Comparison of Brands by Factor Scores",
x = "Factor",
y = "Mean Score",
fill = "Brand") +
theme_minimal(base_size = 14)
print(PAF_promax$loadings, cutoff = 0.3, sort = TRUE)
# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤–æ–π –∫–æ–¥ –±—Ä–µ–Ω–¥–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–µ—Ç–∫—É
Factor_means_long <- Factor_means %>%
mutate(brandrec = recode_factor(
brandrec,
`1` = "Apple",
`2` = "Samsung",
`3` = "LG",
`4` = "Motorola",
`5` = "Other"
)) %>%
pivot_longer(cols = starts_with("RC"), names_to = "Factor", values_to = "Mean")
# –ö—Ä–∞—Å–∏–≤—ã–π –≥—Ä–∞—Ñ–∏–∫
ggplot(Factor_means_long, aes(x = Factor, y = Mean, fill = brandrec)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
scale_fill_brewer(palette = "Set2", name = "Brand") +  # –º—è–≥–∫–∏–µ —Ü–≤–µ—Ç–∞ –∏ –ø–æ–¥–ø–∏—Å—å
labs(
title = "üì± Comparison of Factor Scores by Brand",
subtitle = "Mean scores for each factor across smartphone brands",
x = "Quality Factor",
y = "Average Score"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 16),
plot.subtitle = element_text(size = 12, margin = margin(b = 10)),
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "bottom"
)
# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤–æ–π –∫–æ–¥ –±—Ä–µ–Ω–¥–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è
Factor_means_long <- Factor_means %>%
mutate(brandrec = recode_factor(
brandrec,
`1` = "Apple",
`2` = "Samsung",
`3` = "LG",
`4` = "Motorola",
`5` = "Other"
)) %>%
pivot_longer(cols = starts_with("RC"), names_to = "Factor", values_to = "Mean")
# –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å –ø–æ–¥–ø–∏—Å—è–º–∏ –∑–Ω–∞—á–µ–Ω–∏–π
ggplot(Factor_means_long, aes(x = Factor, y = Mean, fill = brandrec)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
geom_text(aes(label = round(Mean, 2)),
position = position_dodge(width = 0.8),
vjust = -0.5, size = 3.5) +
scale_fill_brewer(palette = "Set2", name = "Brand") +
labs(
title = "Comparison of Factor Scores by Brand",
subtitle = "Mean scores for each factor across smartphone brands",
x = "Quality Factor",
y = "Average Score"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 16),
plot.subtitle = element_text(size = 12, margin = margin(b = 10)),
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "bottom"
)
