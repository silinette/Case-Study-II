---
title: "Case II — Smartphone Quality. Factor Analysis"
author: "Lina Simonian & Elena Pshenichnikova"
date: "2025-03-25"
output:
  html_document:
    theme: flatly
    highlight: breezedark
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    df_print: paged
  pdf_document:
    toc: true
---

```{=html}
<style>
/* Floating TOC container */
.tocify {
  max-width: 300px;
  font-family: 'Segoe UI', sans-serif;
  border-radius: 10px;
  background-color: #f9f9f9;
  box-shadow: 0 6px 15px rgba(0, 0, 0, 0.1);
  padding: 16px 10px;
  margin-top: 20px;
}

/* Section headers inside TOC */
.tocify-header {
  font-weight: bold;
  color: #2c3e50;
  margin: 10px 0 5px 0;
}

/* TOC links */
.tocify-item {
  color: #2c3e50;
  padding: 6px 10px;
  font-size: 14px;
  border-radius: 4px;
  transition: all 0.2s;
}

/* Hover effect */
.tocify-item:hover {
  background-color: #ecf0f1;
  padding-left: 18px;
}

/* Highlighted active section */
.tocify-item.active {
  background-color: #d1ecf1;
  font-weight: bold;
  border-left: 4px solid #3498db;
}
</style>
```
# **Introduction**

This project investigates how customers perceive the quality of smartphones across multiple brands, with the goal of identifying the key dimensions of perceived quality that influence consumer behavior—such as **willingness to pay a premium** or **repurchase intentions**.

Although product quality is widely accepted as a *multidimensional concept*, few studies have successfully captured this complexity from the customer’s point of view using consistent measurement tools. To address this gap, we analyze survey data from over **1,000 smartphone users** in the United States, focusing on **33 product quality items** developed through expert interviews and validation.

Using **factor analysis**, we aim to uncover the underlying structure of how users assess smartphone quality. These latent dimensions are then evaluated across brands to compare perceived strengths and weaknesses. The findings offer valuable insights for **product managers**, **marketers**, and **brand strategists** looking to better understand what drives **customer satisfaction and loyalty** in a competitive market.

------------------------------------------------------------------------

# **Question 1: Orthogonal Factor Analysis**

To begin the analysis, we first **cleaned the dataset** using **listwise deletion**, removing observations with any missing values in the relevant variables.

We then loaded all **required R packages**, including:

-   `corrplot` – for correlation matrix visualization\
-   `olsrr` – for multicollinearity diagnostics (VIF and tolerance values)\
-   `REdaS` – for Bartlett’s test and KMO measure\
-   `psych` – for principal component analysis (PCA)

> Additionally, the dataset was uploaded to **GitHub**, ensuring collaborative access and making it easier to update the project, rerun the analysis, and track progress consistently across devices.

```{r}

# Сlear the database
rm(list=ls()) 
graphics.off()

```

```{r,message=FALSE, warning=FALSE}

#Readability in HTML report
#install.packages("DT")
library(DT)

# Clear the database
rm(list=ls()) 

#Install necessary packages. Delete # sign in order to download all of them. 
#install.packages('rcompanion', repos = "https://cloud.r-project.org")
#install.packages('nortest', repos = "https://cloud.r-project.org")
#install.packages('corrplot', repos = "https://cloud.r-project.org")
#install.packages('olsrr', repos = "https://cloud.r-project.org")
#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")
#install.packages("GPArotation", repos = "https://cloud.r-project.org")

# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO
library(GPArotation)

```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}

# Read the dataset directly from the provided URL
url <- "https://raw.githubusercontent.com/silinette/Case-Study-II/refs/heads/main/Data%20File_Case_Study_Factor%20Analysis_MD.csv"

Data <- read.csv(url, header = TRUE)
Data
```

------------------------------------------------------------------------

## **Data Structure Exploration**

In the next step, we examined the **structure of the dataset** to gain an initial understanding of the available information. We confirmed that the dataset contained **1,014 observations** and **74 variables**.

To ensure proper familiarity with the data, we performed the following steps:\
- Reviewed the **first few rows** of the dataset\
- Explored the **names of all variables**\
- Inspected the **data types** and **sample values** for each column

This exploratory step was essential to validate the integrity of the data before proceeding with any statistical analysis.

------------------------------------------------------------------------

```{r, message=FALSE, warning=FALSE}
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
```

------------------------------------------------------------------------

## **Data Refinement and Cleaning**

At this stage, we refined the dataset by selecting only the variables essential for the analysis — specifically, **33 quality perception items** (`qd1` to `qd33`) and several outcome-related variables such as **brand recommendation**, **willingness to pay**, and **repurchase intentions** (`brandrec`, `wtp1`, `wtp2`, `wtp3`, `ri1`, `ri2`).

To maintain data consistency and integrity, we applied **listwise deletion**, excluding all observations that had missing values in any of the selected fields. This ensured that the resulting dataset was **clean**, **complete**, and suitable for conducting the factor analysis.

------------------------------------------------------------------------

```{r, message=FALSE, warning=FALSE}
# Select only variables needed for the analysis
Data <- Data %>%
  select(
    qd1, qd2, qd3, qd4, qd5, qd6, qd7, qd8, qd9,
    qd10, qd11, qd12, qd13, qd14, qd15, qd16, qd17, qd18, qd19,
    qd20, qd21, qd22, qd23, qd24, qd25, qd26, qd27, qd28, qd29,
    qd30, qd31, qd32, qd33,
    brandrec, wtp1, wtp2, wtp3, ri1, ri2
  )
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na 
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values

# Remove all rows with missing values (listwise deletion)
Data_clean <- na.omit(Data)

# Сheck structure and dimensions of the new dataset
str(Data_clean)
dim(Data_clean)

sum(is.na(Data_clean))

#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE) 
```

## **Descriptive statistics**

To gain an initial understanding of the dataset, we conducted a descriptive statistical analysis for all 33 quality-related survey items (qd1–qd33). These items reflect various dimensions of perceived smartphone quality, such as durability, reliability, aesthetics, ease of use, and performance.

The analysis included measures such as:

-   Mean — showing the average perception for each item
-   Standard deviation — indicating how much responses varied across users
-   Minimum and maximum values — to capture the range of responses
-   Skewness patterns — to identify whether responses were clustered toward low or high values

```{r}
# Descriptive statistics for 33 quality-related variables
# Descriptive statistics (mean, standard deviation, min, max, etc.) for the 33 quality-related survey items (qd1 to qd33) from the cleaned dataset. The results are rounded to 2 decimal places for readability. 
# Normality test not included (norm = TRUE) because Likert-scale items are not truly continuous.
round(stat.desc(Data_clean[, paste0("qd", 1:33)]), digits = 3)
```

## **Visual Inspection of Response Distributions**

The chart above presents **individual bar plots** for all 33 survey items related to perceived product quality (qd1–qd33). Each plot displays the **frequency distribution** of participant responses on a **Likert scale** ranging from 1 to 7.

These visualizations allow us to identify general trends, skewness, and patterns across items. For instance, the majority of responses for most items cluster around values 3 to 5, indicating moderate to high perceptions of quality. Items such as `qd4`, `qd23`, and `qd28` exhibit greater variability or skew, suggesting diverse customer opinions in those areas.

By inspecting these distributions, we can preliminarily assess **data suitability** for factor analysis and detect any **floor or ceiling effects** that could impact the results.

```{r}
# Reshape only qd1 to qd33 into long format for visualization
data_long <- Data_clean %>%
  select(paste0("qd", 1:33)) %>%  # Use only quality-related survey items
  pivot_longer(everything(), names_to = "Item", values_to = "Response")  # Convert from wide to long format

# Order items numerically (qd1 to qd33) instead of alphabetically
data_long$Item <- factor(data_long$Item, levels = paste0("qd", 1:33))

# Preview reshaped data
head(data_long) # Show the first few rows of the long-format data to check the transformation

# Convert responses to ordered factor
data_long$Response <- factor(data_long$Response, levels = sort(unique(unlist(Data_clean)))) # Convert all numeric responses into an ordered factor to preserve Likert scale order in plots

# Plot response distributions for all items (directly in R Markdown)
ggplot(data_long, aes(x = Response)) + # Set x-axis to response values
  geom_bar(fill = "violetred2") + # Create bar charts
  facet_wrap(~ Item, scales = "free_y") + # One plot per item
  labs(title = "Distributions", x = "Response", y = "Count") + # Title and axis labels
  theme_minimal(base_size = 6)  # Clean theme

```

## **Correlation Matrix**

**Correlation Matrix** The correlation matrix shows how strongly each of the 33 quality-related survey items is **linearly** related to the others. It helps identify clusters of items that may measure the same underlying dimension of perceived quality, serving as a foundation for factor analysis

```{r, echo=FALSE, message = FALSE}

# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])

corrplot(raqMatrix,
         method = "color",
         type = "upper",
         tl.cex = 0.5,
         number.cex = 0.5,
         tl.col = "black",   
         mar = c(1,1,1,1),
         cl.cex = 0.5)

```

During the inspection of the correlation matrix, we observed an extremely week correlation between **Item 23 (qd23)** and **Item 4 (qd4)**. Such week multicollinearity indicates that these items are not likely measuring the same underlying concept, potentially distorting the factor structure.\

## **Bartlett’s Test и KMO**

```{r, echo=FALSE, message = FALSE}
# Run Bartlett's test of sphericity
bart_spher(Data_clean[, paste0("qd", 1:33)])

#KMO Criterion
kmo_test <- KMOS(Data_clean[, paste0("qd", 1:33)])
kmo_test$KMO           # Overall KMO measure

#Anti-Image Correlations
sort(kmo_test$MSA)     # Measure of sampling adequacy for each item

```

**Bartlett's Test of Sphericity** checks if the correlation matrix is an identity matrix (no relationships between variables). A significant result (p \< 0.05) means the variables do have meaningful correlations, which supports using factor analysis or PCA.

**Kaiser–Meyer–Olkin (KMO) Measure** assesses the adequacy of sampling for factor analysis by examining how patterns of correlations are relatively compact. The KMO value ranges from 0 to 1, where a value closer to 1 indicates that a factor analysis is appropriate. A KMO of 0.9493879 is considered excellent, suggesting that the data are highly suitable for factor analysis or PCA.

**Anti-Image Correlations.** Although qd4 still meet the minimum threshold (above 0.50), its anti-image correlation values are relatively lower compared to the rest. However, qd23 despite a weak visual correlation pattern, shows **good MSA** (0.90).

Since we observed controversial results for qd4 and especially for qd23—with a strong MSA but weak visual correlationit is important to assess its communality to determine how well it is represented by the factor solution.

```{r, echo=FALSE, message=FALSE}

# Running PCA without rotation for scree plot analysis
PC <- principal(
  Data_clean[, paste0("qd", 1:33)],
  rotate = "none"
)
# Extraction and displaying communalities
PC_communalities <- data.frame(sort(PC$communality))
PC_communalities
```

**Communality** measures how much of a variable’s variance is explained by the extracted factors. Values closer to 1 indicate a good fit, while low values suggest the item is not well captured by the model.

In this case, qd23 has a communality of only 0.046, indicating that less than 5% of its variance is accounted for by the factors. qd4 performs even worse, with a communality of just 0.004.

**Conclusion:** qd4 qd23 have low communality, confirming that it does not contribute meaningfully to the factor structure. Therefore, both qd4 and qd23 should be excluded from the final analysis to improve the overall model quality.

## **Scree Plot**

```{r, echo=FALSE, message=FALSE}

# Running PCA without rotation for scree plot analysis
PC0 <- principal(
  Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
  rotate = "none"
)

PC0$values #Eigenvalues

# Scree plot of eigenvalues
plot(PC0$values, type = "b", main = "Scree Plot", xlab = "Factor Number", ylab = "Eigenvalue")
abline(h = 1) # Kaiser criterion line

# Extraction and displaying communalities
PC0_communalities <- data.frame(sort(PC0$communality))
PC0_communalities

```

The **scree plot** displays the eigenvalues of all extracted factors. The first factor explains a substantial amount of variance, while the next four also have eigenvalues above 1. From the sixth factor onward, the curve levels off and eigenvalues drop below the threshold.

Based on the **Kaiser criterion** (eigenvalues \> 1) and the **elbow rule**, it is decided to retain **eight factors**, as they represent the most meaningful structure in the data.

## **Principal Component Analysis with Varimax Rotation**

Based on the scree plot and initial inspection of eigenvalues, a solution with **8** factors seems appropriate. To further explore the structure of the data and validate this assumption, a **Principal Component Analysis (PCA)** with **Varimax rotation** will now be conducted.

```{r, echo=FALSE, message = FALSE}

# Principal Components Analysis (PCA) with Varimax Rotation
PCA1 <- principal(
  Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
  rotate = "varimax",
  nfactors = 8,
  scores = TRUE
)


# Sort and save communalities
PCA1_communalities <- data.frame(sort(PCA1$communality))
PCA1_communalities

```
Communalities from the PCA with Varimax rotation showed that items were well explained by the retained 8-factor solution. Variables had high communalities ( > 0.70), indicating strong representation in the factor structure. 

```{r, echo=FALSE,  message=FALSE, warning=FALSE}
### **Variance Explained Table for PCA Components**
# Extract eigenvalues
EigenValue_PCA1 <- PCA1$values

# Extraction eigenvalues from the PCA object
EigenValue <- PCA1$values

# Number of items (qd1..qd33) used in factor analysis
n_items <- 31

# Calculating percentage of variance explained by each factor
Variance <- EigenValue / n_items * 100

# Cumulative variance explained
SumVariance <- cumsum(Variance)

# Building a summary table of eigenvalues and variance explained
Total_Variance_Explained <- cbind(
  EigenValue = EigenValue[EigenValue > 0],
  Variance = Variance[Variance > 0],
  Total_Variance = SumVariance[Variance > 0]
)

# Viewing the table
Total_Variance_Explained

```
A Principal Component Analysis (PCA) was conducted on 31 survey items. The first component explained 51.16% of the total variance, indicating a dominant underlying dimension. The first eight components together accounted for 84.11% of the total variance, supporting the adequacy of an 8-factor solution. The remaining components contributed minimally and were not retained for further interpretation. This level of explained variance suggests that the factor model provides a robust and meaningful reduction of the original variable set.
```{r, echo=FALSE, message = FALSE}

# Print factor loadings above 0.3, sorted by factor
# A threshold of 0.3 is commonly used to highlight meaningful factor-variable relationships
print(PCA1$loadings, cutoff = 0.3, sort = TRUE)

```
Each factor groups together items with high loadings, reflecting distinct psychological or product-related dimensions. Factor names were assigned based on the thematic content of the items with the strongest loadings.
```{r, echo=FALSE, message = FALSE}
library(knitr)

factor_table <- data.frame(
  Factor = c("RC1", "RC2", "RC3", "RC4", "RC5", "RC6", "RC7", "RC8"),
  Quality_Dimension = c(
    "Performance",
    "Serviceability",
    "Aesthetics/Appearance",
    "Durability",
    "Ease of Use",
    "Features/Versatility",
    "Reliability/Flawlessness",
    "Distinctiveness/Prestige"
  ),
  Survey_Items = c(
    "qd1, qd10, qd20, qd27",                   # RC1
    "qd9, qd14, qd19, qd21, qd24",             # RC2
    "qd22, qd29, qd33",                        # RC3
    "qd15, qd17, qd32",                        # RC4
    "qd3, qd11, qd13, qd30",                   # RC5
    "qd6, qd8, qd18, qd25",                    # RC6
    "qd2, qd5, qd7, qd12, qd16",               # RC7
    "qd26, qd28, qd31"                         # RC8
  )
)

kable(factor_table, caption = "Table: Factors and Their Interpretations Based on PCA (Varimax) Loadings", align = "l")


```

## **Principal Axis Factoring (PAF) with Varimax rotation**

```{r, echo=FALSE, message = FALSE}
# Run Principal Axis Factoring (PAF) with Varimax rotation
PAF1 <- fa(
  Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
  nfactors = 8,
  rotate = "varimax",
  fm = "pa",           # метод extraction = Principal Axis
  scores = TRUE
)


# Sort and save communalities
PAF1_communalities <- data.frame(sort(PAF1$communality))
PAF1_communalities
```
The communalities obtained from the Principal Axis Factoring (PAF) model indicate how well each survey item is explained by the extracted factors. In this analysis, the majority of items showed moderate to high communalities, with values starting from 0.63. These values suggest that these items share substantial common variance with other items and are well represented within the factor structure.

```{r, echo=FALSE,  message=FALSE, warning=FALSE}
### **Variance Explained Table for PCA Components**
# Extract eigenvalues
EigenValue_PAF1 <- PAF1$e.values

# Calculating percentage of variance explained by each factor
Variance_PAF1 <- EigenValue_PAF1 / n_items * 100

# Cumulative variance explained
SumVariance_PAF1 <- cumsum(Variance)

# Building a summary table of eigenvalues and variance explained
Total_Variance_Explained_PAF1 <- cbind(
  EigenValue_PAF1 = EigenValue_PAF1[EigenValue > 0],
  Variance_PAF1 = Variance_PAF1[Variance > 0],
  Total_Variance_PAF1 = SumVariance_PAF1[Variance > 0]
)

# Viewing the table
Total_Variance_Explained_PAF1

```
The Principal Axis Factoring (PAF) model produced an eigenvalue and variance structure closely aligned with that of the Principal Component Analysis (PCA). The first factor in both models explained 51.16% of the total variance, confirming the presence of a dominant underlying dimension in the data. Across the first eight components, PAF explained a cumulative 84.1% of the total variance, identical to the PCA solution, indicating strong agreement in the overall factor structure.


```{r, echo=FALSE, message = FALSE}

# Print factor loadings above 0.3, sorted by factor
# A threshold of 0.3 is commonly used to highlight meaningful factor-variable relationships
print(PAF1$loadings, cutoff = 0.3, sort = TRUE)

```
Each factor was defined by items with strong loadings (≥ 0.30), with minimal cross-loading. Key groupings—such as qd9, qd14, qd19, and qd24 on PA2, and qd2, qd5, qd7, and qd12 on PA1—reflected consistent themes like serviceability and style consciousness.
```{r,echo=FALSE, message = FALSE}
factor_table <- data.frame(
  Factor = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8"),
  Quality_Dimension = c(
    "Serviceability",
    "Performance",
    "Reliability/Flawlessness",
    "Ease of Use",
    "Aesthetics/Appearance",
    "Features/Versatility",
    "Durability",
    "Distinctiveness/Prestige"
  ),
  Survey_Items = c(
    "qd2, qd5, qd7, qd12, qd16",               # PA1
    "qd9, qd14, qd19, qd21, qd24",             # PA2
    "qd22, qd29, qd33",                        # PA3
    "qd6, qd8, qd18, qd25",                    # PA4
    "qd3, qd11, qd13, qd30",                   # PA5
    "qd15, qd17, qd32",                        # PA6
    "qd1, qd10, qd20, qd27",                   # PA7
    "qd26, qd28, qd31"                         # PA8
  )
)
kable(factor_table, caption = "Table: Factors and Their Interpretations Based on PAF (Varimax) Loadings", align = "l")
```
The PCA and PAF models yielded highly consistent 8-factor solutions, with most items loading on the same factors across both methods. Key dimensions like Distinctiveness/Prestige, Ease of Use, Aesthetics, and Reliability were stable. Minor differences appeared in factor ordering (e.g., Performance and Serviceability switched positions), but item groupings remained unchanged. 

## **PCA and PAF comparison**

```{r, echo=FALSE, message = FALSE}
#PCA
Eigen_PCA <- PCA1$values
Variance_PCA <- Eigen_PCA / 31 * 100
CumVar_PCA <- cumsum(Variance_PCA)

#PAF
Eigen_PAF <- PAF1$e.values
Variance_PAF <- Eigen_PAF / 31 * 100
CumVar_PAF <- cumsum(Variance_PAF)

# Объединяем пары рядом
Comparison_Table <- data.frame(
  Factor = 1:length(Eigen_PCA),
  Eigen_PCA = round(Eigen_PCA, 3),
  Eigen_PAF = round(Eigen_PAF, 3),
  Variance_PCA = round(Variance_PCA, 2),
  Variance_PAF = round(Variance_PAF, 2),
  Cumulative_PCA = round(CumVar_PCA, 2),
  Cumulative_PAF = round(CumVar_PAF, 2)
)

# Красиво выводим
kable(Comparison_Table, caption = "Side-by-Side Comparison of PCA and PAF (Varimax)")

```
The table shows a nearly identical pattern of eigenvalues and explained variance between PCA and PAF for the first 20 factors. Both methods yield the same eigenvalues and cumulative variance up to Factor 8, reaching 84.11% of total variance explained. 
```{r, echo=FALSE, message = FALSE}
# Эстетичная таблица сравнения PCA и PAF факторов

comparison_df <- data.frame(
  Metric = c("SS Loadings", "Proportion of Variance", "Cumulative Variance"),
  
  `RC2 (PCA)` = c(4.673, 0.151, 0.151),
  `PA2 (PAF)` = c(4.569, 0.147, 0.147),
  
  `RC1 (PCA)` = c(3.908, 0.126, 0.277),
  `PA1 (PAF)` = c(3.577, 0.115, 0.263),
  
  `RC5 (PCA)` = c(3.492, 0.113, 0.389),
  `PA5 (PAF)` = c(3.404, 0.110, 0.373),
  
  `RC4 (PCA)` = c(3.297, 0.106, 0.496),
  `PA4 (PAF)` = c(3.121, 0.101, 0.473),
  
  `RC7 (PCA)` = c(3.020, 0.097, 0.593),
  `PA3 (PAF)` = c(2.781, 0.090, 0.563),
  
  `RC6 (PCA)` = c(2.856, 0.092, 0.685),
  `PA7 (PAF)` = c(2.675, 0.086, 0.649),
  
  `RC3 (PCA)` = c(2.492, 0.080, 0.766),
  `PA6 (PAF)` = c(2.324, 0.075, 0.724),
  
  `RC8 (PCA)` = c(2.337, 0.075, 0.841),
  `PA8 (PAF)` = c(1.935, 0.062, 0.787)
)

# Вывод в HTML-стиле (в отчёте будет аккуратно)
knitr::kable(
  comparison_df,
  caption = "Side-by-Side Comparison of Factor Metrics from PCA and PAF (Varimax)",
  align = c("l", rep("c", ncol(comparison_df) - 1))
)
```
The comparison shows very similar SS loadings, proportion of variance, and cumulative variance for each factor across PCA and PAF. While PCA slightly overestimates the variance explained (as it includes unique variance), PAF presents a more conservative estimate by focusing on shared variance only. Despite this, the difference is minimal. This further supports the robustness of the 8-factor structure.

# **Question 2: Oblique Factor Analysis**

## **Principal Axis Factoring (PAF) with Promax rotation**

```{r}
# Run Principal Axis Factoring with oblique (Promax) rotation
PAF_promax <- fa(
  Data_clean[, paste0("qd", setdiff(1:33, c(4, 23)))],
  nfactors = 8,
  rotate = "promax",
  fm = "pa",
  scores = TRUE
)

# Extract eigenvalues
EigenValue_PAF_promax <- PAF_promax$e.values
EigenValue_PAF_promax

# Communalities
PAF_promax_communalities <- data.frame(sort(PAF_promax$communality))
PAF_promax_communalities

```

```{r, echo=FALSE, message = FALSE}
print(PAF_promax$loadings, cutoff = 0.3, sort = TRUE)
```

```{r}
library(knitr)

factor_table <- data.frame(
  Factor = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8"),
  
  `Quality Dimension` = c(
    "Performance",
    "Features / Versatility",
    "Reliability / Flawlessness",
    "Durability",
    "Ease of Use",
    "Serviceability",
    "Aesthetics / Appearance",
    "Prestige / Distinctiveness"
  ),
  
  `Survey Items` = c(
    "qd2, qd5, qd7, qd12, qd16",         
    "qd6, qd8, qd18, qd25",              
    "qd22, qd29, qd33",                  
    "qd15, qd17, qd26, qd28, qd31, qd32",
    "qd3, qd11, qd13, qd30",             
    "qd9, qd14, qd19, qd21, qd24",       
    "qd1, qd10, qd20, qd27",             
    "qd4, qd23"                          
  ),
  
  `Brief Description` = c(
    "How well the smartphone performs its core tasks — efficiency, speed, and consistency.",
    "The number, innovativeness, and usefulness of extra features.",
    "Frequency and severity of defects or malfunctions; likelihood of failure.",
    "Material quality and the product’s lifespan under regular or heavy use.",
    "How easy it is to learn and operate the phone and its functionalities.",
    "Quality of customer service — accessibility, responsiveness, professionalism.",
    "Visual and tactile appeal of the smartphone — design, feel, finish.",
    "The uniqueness and symbolic value of the phone as a status-enhancing product."
  )
)

kable(factor_table, caption = "Table: Factors and Their Interpretations Based on PAF (Promax) Loadings", align = "l")

```

```{r, echo=FALSE, message = FALSE}
# Add factor ratings to the raw data
Data_with_scores <- cbind(Data_clean, PCA1$scores)

# Let’s look at brand names (suppose the variable is called brandrec)
unique(Data_with_scores$brandrec)  

# Compare average factor values by brand
library(dplyr)

factor_by_brandrec <- Data_with_scores %>%
 group_by(brandrec) %>%
  summarise(across(RC1:RC8, mean, na.rm = TRUE))

#print(factor_by_brandrec)
```

```{r, echo=FALSE, message = FALSE}

# Combine factors and brands into one dataframe
Data_scores <- cbind(Data_clean["brandrec"], PCA1$scores)

# Group by brand and calculate the average value for each factor
Factor_means <- Data_scores %>%
  group_by(brandrec) %>%
  summarise(across(starts_with("RC"), mean))

Factor_means

```

```{r, echo=FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(forcats)

# Преобразование таблицы в длинный формат
Factor_means_long <- Factor_means %>%
  mutate(brandrec = recode_factor(
    brandrec,
    `1` = "Apple",
    `2` = "Samsung",
    `3` = "LG",
    `4` = "Motorola",
    `5` = "Other"
  )) %>%
  pivot_longer(cols = starts_with("RC"), names_to = "Factor", values_to = "Mean") %>%
  mutate(Factor = factor(Factor, levels = unique(Factor))) # фиксируем порядок факторов

# График
ggplot(Factor_means_long, aes(x = Factor, y = Mean, fill = brandrec)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_text(
    aes(label = round(Mean, 2),
        vjust = ifelse(Mean >= 0, -0.3, 1.2)),
    position = position_dodge(width = 0.8),
    size = 3.5
  ) +
  scale_fill_brewer(palette = "Set2", name = "Brand") +
  labs(
    title = "Comparison of Factor Scores by Brand",
    subtitle = "Mean scores for each factor across smartphone brands",
    x = "Quality Factor",
    y = "Average Score"
  ) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12, margin = margin(b = 10)),
    axis.text.x = element_text(angle = 20, hjust = 1),
    legend.position = "bottom"
  )


```
