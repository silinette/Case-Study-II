---
title: "Case II — Smartphone Quality (Factor Analysis)"
author: "Lina Simonian & Elena Pshenichnikova"
output: html_document
date: "2025-03-27"
---
### Introduction

The goal of this project is to explore how customers perceive smartphone quality and to identify the key dimensions (factors) that make up these perceptions. Traditional customer satisfaction surveys have failed to predict actual customer behavior, such as willingness to pay a price premium or repurchase intentions. Therefore, a factor analysis approach is used to uncover the latent dimensions of perceived product quality based on 33 survey items. These factors will then be used to explain and predict key behavioral outcomes, and to compare performance between major smartphone brands.

```{r, include=FALSE}

# Сlear the database
rm(list=ls()) 
graphics.off()

```

```{r setup, include=FALSE, results = "hide"}

#Readability in HTML report
#install.packages("DT")
library(DT)

# Clear the database
rm(list=ls()) 

#Install necessary packages. Delete # sign in order to download all of them. 
#install.packages('rcompanion', repos = "https://cloud.r-project.org")
#install.packages('nortest', repos = "https://cloud.r-project.org")
#install.packages('corrplot', repos = "https://cloud.r-project.org")
#install.packages('olsrr', repos = "https://cloud.r-project.org")
#install.packages('dplyr', repos = "https://cloud.r-project.org")
#install.packages('pastecs', repos = "https://cloud.r-project.org")
#install.packages('REdaS', repos = "https://cloud.r-project.org")
#install.packages('psych', repos = "https://cloud.r-project.org")
#install.packages('lm.beta', repos = "https://cloud.r-project.org")


# Load required libraries
library(ggplot2)
library(dplyr)
library(readr)
library(broom)
library(lm.beta)
library(pastecs)
library(tidyr)
library(corrplot)     # For correlation matrix visualization
library(olsrr)        # For VIF and tolerance values
library(REdaS)        # For Bartlett's test
library(psych)        # For PCA and KMO

```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}

# Read the dataset directly from the provided URL
url <- "https://raw.githubusercontent.com/silinette/Case-Study-II/refs/heads/main/Data%20File_Case_Study_Factor%20Analysis_MD.csv"

Data <- read.csv(url, header = TRUE)
Data
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Explore the structure of the dataset
dim(Data) # Check the number of rows and columns in the dataset
head(Data) # Display the first few rows of the dataset
str(Data) # Display the structure of the dataset (types, sample values)
names(Data) # Show all variable names in the dataset
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Check for missing values
list_na <- colnames(Data)[apply(Data, 2, anyNA)] # Identify columns with missing values
list_na 
sum(is.na(Data)) # Count total number of missing values
sum(!is.na(Data)) # Count total number of non-missing values

#Column names that we do not touch/Define variables we will keep even with NAs:
cols_keep_na <- c("noOfDefects", "defectsWarranty", "fRecall",
                  "def_sev1", "def_sev2", "didComplain",
                  "reactionComplain", "nextPurchase")

# Create column vector to check for clearances/Define variables to check for complete cases:
cols_check <- setdiff(names(Data), cols_keep_na)

# Delete the rows where there is NA in these columns (the rest are not touched):
Data_clean <- Data[complete.cases(Data[, cols_check]), ]
Data_clean

#write.csv(Data_clean, file = "~/Desktop/Data_clean.csv", row.names = FALSE)

```
#Descriptive statistics 
```{r}

# Descriptive statistics for 33 quality-related variables
round(stat.desc(Data_clean[, paste0("qd", 1:33)]), digits = 2)

# Descriptive statistics (mean, standard deviation, min, max, etc.) for the 33 quality-related survey items (qd1 to qd33) from the cleaned dataset. The results are rounded to 2 decimal places for readability. 

# Normality test not included (norm = TRUE) because Likert-scale items are not truly continuous.
```
```{r}
# Reshape the data into long format for visualization
data_long <- Data_clean %>%
  pivot_longer(everything(), names_to = "Item", values_to = "Response") # Convert the wide-format data frame into long format, where each row represents one response to one item.

# Preview reshaped data
head(data_long) # Show the first few rows of the long-format data to check the transformation

# Convert responses to ordered factor
data_long$Response <- factor(data_long$Response, levels = sort(unique(unlist(Data_clean)))) # Convert all numeric responses into an ordered factor to preserve Likert scale order in plots

# Save barplots to PDF for visual inspection
pdf("~/Desktop/Distributions_by_Item.pdf", width = 30, height = 30)

# Plot response distributions for all items
ggplot(data_long, aes(x = Response)) + # Set x-axis to response values
  geom_bar(fill = "steelblue") + # Create blue bar charts
  facet_wrap(~ Item, scales = "free_y") + # Create separate plot for each item
  labs(title = "Distributions", x = "Response", y = "Count") + # Add title and axis labels
  theme_minimal()  # Use a clean and minimal theme for the plot

dev.off()
# Sorry, not enough space in R markdown :( 
```

#Correlation Matrix
```{r, echo=FALSE, message = FALSE}

# Calculate and visualize correlation matrix
raqMatrix <- cor(Data_clean[, paste0("qd", 1:33)])
corrplot(as.matrix(raqMatrix))

```
#Bartlett’s Test и KMO
```{r, echo=FALSE, message = FALSE}

# Run Bartlett's test of sphericity
bart_spher(Data_clean[, paste0("qd", 1:33)])

#KMO Criterion
kmo_test <- KMOS(Data_clean[, paste0("qd", 1:33)])
kmo_test$KMO           # Overall KMO measure

#Anti-Image Correlations
sort(kmo_test$MSA)     # Measure of sampling adequacy for each item

```
### Scree Plot
```{r, echo=FALSE, message = FALSE}

# Running PCA without rotation for scree plot analysis
PC0 <- principal(Data_clean[, paste0("qd", 1:33)], 
                 rotate = "none")
PC0$values #Eigenvalues

# Scree plot of eigenvalues
plot(PC0$values, type = "b", main = "Scree Plot", xlab = "Factor Number", ylab = "Eigenvalue")
abline(h = 1) # Kaiser criterion line

# Extraction and displaying communalities
PC0_communalities <- data.frame(sort(PC0$communality))
PC0_communalities

```
### PCA with Varimax Rotation
```{r, echo=FALSE, message = FALSE}

# Principal Components Analysis (PCA) with Varimax Rotation
PC1 <- principal(
  Data_clean[, paste0("qd", 1:33)],  # use cleaned data and qd1..qd33 columns
  rotate = "varimax",
  nfactors = 9,
  scores = TRUE
)

# Extract eigenvalues
EigenValue_pc1 <- PC1$values
EigenValue_pc1

# Sort and save communalities
PC1_communalities <- data.frame(sort(PC1$communality))
PC1_communalities

```

```{r, echo=FALSE, message = FALSE}

# Extraction eigenvalues from the PCA object
EigenValue <- PC1$values

# Number of items (qd1..qd33) used in factor analysis
n_items <- 33

# Calculating percentage of variance explained by each factor
Variance <- EigenValue / n_items * 100

# Cumulative variance explained
SumVariance <- cumsum(Variance)

# Building a summary table of eigenvalues and variance explained
Total_Variance_Explained <- cbind(
  EigenValue = EigenValue[EigenValue > 0],
  Variance = Variance[Variance > 0],
  Total_Variance = SumVariance[Variance > 0]
)

# Viewing the table
Total_Variance_Explained

```

```{r, echo=FALSE, message = FALSE}

# Print factor loadings above 0.3, sorted by factor
# A threshold of 0.3 is commonly used to highlight meaningful factor-variable relationships
print(PC1$loadings, cutoff = 0.3, sort = TRUE)

```

model <- lm(y ~ x, data = Data)

```{r, echo=FALSE, message = FALSE}
library(knitr)

factor_table <- data.frame(
  Factor = c("RC1", "RC2", "RC3", "RC4", "RC5", "RC6", "RC7", "RC8", "RC9"),
  Quality_Dimension = c(
    "Performance",
    "Features/Versatility",
    "Reliability/Flawlessness",
    "Durability",
    "Ease of Use",
    "Serviceability",
    "Aesthetics/Appearance",
    "Distinctiveness/Prestige",
    "Conformance (Build Quality)"
  ),
  Survey_Items = c(
    "qd2, qd5, qd7, qd12, qd16",
    "qd6, qd8, qd18, qd25",
    "qd22, qd26, qd29, qd33",
    "qd15, qd17, qd28, qd31",
    "qd3, qd11, qd13, qd30",
    "qd9, qd14, qd19, qd21, qd24",
    "qd1, qd10, qd20",
    "qd4, qd23",
    "qd27, qd32"
  ),
  Interpretation = c(
    "Core performance and operational consistency.",
    "Innovative and exciting features beyond core functionality.",
    "Freedom from defects and malfunctions.",
    "Longevity and resistance to wear and heavy usage.",
    "User-friendliness and ease of operation.",
    "Quality and responsiveness of customer service.",
    "Visual appeal and design of the product.",
    "Uniqueness and symbolic value of the smartphone.",
    "Fit, finish, and use of quality materials."
  )
)

kable(factor_table, caption = "Table: Factors and Their Interpretations Based on Survey Items", align = "l")

```